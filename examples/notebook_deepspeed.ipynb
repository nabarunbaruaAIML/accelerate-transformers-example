{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook launcher example \n",
    "\n",
    "[Launching Multi-Node Training from a Jupyter Environment](https://huggingface.co/docs/accelerate/basic_tutorials/notebook)\n",
    "\n",
    "[How to use DeepSpeed](https://huggingface.co/docs/accelerate/usage_guides/deepspeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepspeed==0.7.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Utils\n",
    "\n",
    "#### load and tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def preprocess(sample, tokenizer=None):\n",
    "    enc = tokenizer(sample[\"text\"], truncation=True)\n",
    "    if \"label\" in sample:\n",
    "        enc[\"labels\"] = sample[\"label\"]\n",
    "    return enc\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset_id, tokenizer=None, preprocess_fn=preprocess):\n",
    "    dataset = load_dataset(dataset_id)\n",
    "    # remove not needed columns\n",
    "    remove_columns = dataset[\"train\"].column_names\n",
    "    if \"labels\" in remove_columns:\n",
    "        remove_columns.remove(\"labels\")\n",
    "    # tokenize dataset\n",
    "    dataset = dataset.map(partial(preprocess, tokenizer=tokenizer), batched=True, remove_columns=remove_columns)\n",
    "\n",
    "    # print some stats\n",
    "    print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "    print(f\"Dataset columns: {dataset['train'].column_names}\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create dataloader function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def create_dataloaders(dataset, collate_fn=None, train_batch_size=8, eval_batch_size=32):\n",
    "    train_dataloader = DataLoader(dataset[\"train\"], collate_fn=collate_fn, shuffle=True, batch_size=train_batch_size)\n",
    "    eval_dataloader = DataLoader(\n",
    "        dataset[\"validation\"], collate_fn=collate_fn, shuffle=False, batch_size=eval_batch_size\n",
    "    )\n",
    "    return train_dataloader, eval_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import transformers\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "from accelerate.utils.deepspeed import DummyOptim, DummyScheduler\n",
    "from transformers import (\n",
    "    get_linear_schedule_with_warmup,\n",
    "    DataCollatorWithPadding,\n",
    "    set_seed,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "from datasets import DatasetDict\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "###### Hyperparameters ######\n",
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    train_batch_size: int = 64\n",
    "    eval_batch_size: int = 64\n",
    "    learning_rate: float = 5e-5\n",
    "    num_epochs: int = 3\n",
    "\n",
    "\n",
    "def training_function(\n",
    "    model: PreTrainedModel,\n",
    "    dataset: DatasetDict,\n",
    "    tokenizer: PreTrainedTokenizer = None,\n",
    "    hp: Hyperparameters = None,\n",
    "):\n",
    "    # dummy optimizer and scheduler for deepspeed\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = DummyOptim(optimizer_grouped_parameters, lr=hp.learning_rate)\n",
    "    lr_scheduler = DummyScheduler(\n",
    "        optimizer,\n",
    "        num_warmup_steps=100,\n",
    "        num_training_steps=len(train_dataloader) * hp.num_epochs,\n",
    "    )\n",
    "    gradient_accumulation_steps = 2\n",
    "    # Initialize accelerator and Deepspeed\n",
    "    # deepspeed needs to know your gradient accumulation steps before hand, so don't forget to pass it\n",
    "    # Remember you still need to do gradient accumulation by yourself, just like you would have done without deepspeed\n",
    "    deepspeed_plugin = DeepSpeedPlugin(\n",
    "        zero_stage=2,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        offload_optimizer_device=\"cpu\",\n",
    "        offload_param_device=\"cpu\",\n",
    "    )\n",
    "    accelerator = Accelerator(\n",
    "        fp16=True, deepspeed_plugin=deepspeed_plugin, gradient_accumulation_steps=gradient_accumulation_steps\n",
    "    )\n",
    "\n",
    "    # To have only one message (and not 8) per logs of Transformers or Datasets, we set the logging verbosity\n",
    "    # to INFO for the main process only.\n",
    "    if accelerator.is_main_process:\n",
    "        datasets.utils.logging.set_verbosity_warning()\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
    "\n",
    "    train_dataloader, eval_dataloader = create_dataloaders(\n",
    "        dataset, collate_fn=data_collator, train_batch_size=hp.train_batch_size, eval_batch_size=hp.eval_batch_size\n",
    "    )\n",
    "    # The seed need to be set before we instantiate the model, as it will determine the random head.\n",
    "    set_seed(34)\n",
    "\n",
    "    # Prepare everything\n",
    "    model, optimizer, lr_scheduler, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model, optimizer, lr_scheduler, train_dataloader, eval_dataloader\n",
    "    )\n",
    "\n",
    "    # Instantiate a progress bar to keep track of training. Note that we only enable it on the main\n",
    "    progress_bar = tqdm(range(hp.num_epochs * len(train_dataloader)), disable=not accelerator.is_main_process)\n",
    "    # Now we train the model\n",
    "    for epoch in range(hp.num_epochs):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # perform gradient accumulation\n",
    "            with accelerator.accumulate(model):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                progress_bar.update(1)\n",
    "\n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "            # We gather predictions and labels from the 8 TPUs to have them all.\n",
    "            all_predictions.append(accelerator.gather(predictions))\n",
    "            all_labels.append(accelerator.gather(batch[\"labels\"]))\n",
    "\n",
    "        # Concatenate all predictions and labels.\n",
    "        # The last thing we need to do is to truncate the predictions and labels we concatenated\n",
    "        # together as the prepared evaluation dataloader has a little bit more elements to make\n",
    "        # batches of the same size on each process.\n",
    "        all_predictions = torch.cat(all_predictions)[: len(dataset[\"validation\"])]\n",
    "        all_labels = torch.cat(all_labels)[: len(dataset[\"validation\"])]\n",
    "\n",
    "        # Use accelerator.print to print only on the main process.\n",
    "        metric = evaluate.load(\"accuracy\")\n",
    "        eval_metric = metric.compute(predictions=all_predictions, references=all_labels)\n",
    "        accelerator.print(f\"epoch {epoch+1}:\", eval_metric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup, DataCollatorWithPadding\n",
    "\n",
    "\n",
    "###### Load model and dataset ######\n",
    "model_id = \"distilbert-base-uncased\"\n",
    "dataset_id = \"emotion\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=6)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "dataset = tokenize_dataset(dataset_id, tokenizer)\n",
    "hyperparameters = Hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "launch  training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 250/750 [01:11<02:33,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'accuracy': 0.929}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 500/750 [02:29<01:16,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: {'accuracy': 0.934}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [03:53<00:00,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: {'accuracy': 0.934}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "args = (model, dataset, tokenizer, hyperparameters)\n",
    "\n",
    "notebook_launcher(training_function, args, mixed_precision=\"fp16\", num_processes=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6dd96c16031089903d5a31ec148b80aeb0d39c32affb1a1080393235fbfa2fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
